using Distances

###### Parameters ######
actions = ['R', 'L', 'U', 'D']
discount = 0.01
Q_table = Dict()
learning_rate = 0.01
exploration_rate = 0.01
###### ######### ######


mutable struct MaximumLikelihoodMDP
	ğ’® # state space (assumes 1:nstates)
	ğ’œ # action space (assumes 1:nactions)
	N # transition count N(s,a,sâ€²)
	Ï # reward sum Ï(s, a)
	Î³ # discount
	U # value function
	planner
 end
 
 function lookahead(model::MaximumLikelihoodMDP, s, a)
	ğ’®, U, Î³ = model.ğ’®, model.U, model.Î³
	n = sum(model.N[s,a,:])
	if n == 0
	   return 0.0
	end
	r = model.Ï[s, a] / n
	T(s,a,sâ€²) = model.N[s,a,sâ€²] / n
	return r + Î³ * sum(T(s,a,sâ€²)*U[sâ€²] for sâ€² in ğ’®)
 end
 
 function backup(model::MaximumLikelihoodMDP, U, s)
	return maximum(lookahead(model, s, a) for a in model.ğ’œ)
 end
 
 function update!(model::MaximumLikelihoodMDP, s, a, r, sâ€²)
	model.N[s,a,sâ€²] += 1
	model.Ï[s,a] += r
	update!(model.planner, model, s, a, r, sâ€²)
	return model
 end


mutable struct Token
	position::Array{Int}
	Value::Int64
end


mutable struct Board
	ğ’®::Array{Token}
	size::Int64
end

function update!(model::QLearning, s, a, r, sâ€²)
	Î³, Q, Î± = model.Î³, model.Q, model.Î±
	Q[s, a] += Î± * (r + Î³ * maximum(Q[sâ€², :]) - Q[s, a])
	return model
end

function initialize_game(size = 3)
	ğ’® = ones(Int64, (size, size))

	location_one = [rand(1:size), rand(1:size)]
	location_two = [rand(1:size), rand(1:size)]
	while location_one == location_two
		location_two = [rand(1:size), rand(1:size)]
	end
	ğ’®[location_one[1], location_one[2]] = Bool(rand(0:1)) ? Int64(2) : Int64(4)
	ğ’®[location_two[1], location_two[2]] = Bool(rand(0:1)) ? Int64(2) : Int64(4)
	return ğ’®
end



function slide_rows(board)
	new_board = copy(board)
	for (row,slice) in enumerate(eachrow(board))
		filtered_array = filter(x -> x != 1, slice)
		result = []
		i = length(filtered_array)
		while i >= 1
			if i > 1 && filtered_array[i] == filtered_array[i - 1]
				pushfirst!(result, filtered_array[i] * 2)
				i -= 1
			else
				pushfirst!(result, filtered_array[i])
			end
			i -= 1
		end
    # Rellenar con ceros hasta alcanzar el tamaÃ±o original del array
    while length(result) < length(slice)
        pushfirst!(result, 1)

    end

		new_board[row,:] = result
	end
	return new_board
end


function calculate_rewards(board,new_board)
	return euclidean(board,new_board)

end

function move(board, action, return_rewards = true)
	if action == 'L'
		rotated_board = reverse(board,dims=2)
	end
	if action == 'U'
		rotated_board = copy(reverse(board',dims=2))
	end
	if action == 'D'
		rotated_board = board'
	end


	slided_board = slide_rows(board)

	if action == 'L'
		rerotated_board = reverse(slided_board,dims=2)
	end
	if action == 'U'
		rerotated_board = copy(reverse(slided_board,dims=2)')
	end
	if action == 'D'
		rerotated_board = slided_board'
	end
	display(board)
	display(rerotated_board)

	rewards = calculate_rewards(board,rerotated_board)

	print(rewards)

	if return_rewards
		return rerotated_board, rewards
	end
end






board = initialize_game(4)

move(board,'L')






model = MaximumLikelihoodMDP(board,actions,discount,Q_table,learning_rate,exploration_rate)
function run(model)
	while max(board) != 2048
		action = lookahead(model.board,model.exploration_rate)

		break
	end

end